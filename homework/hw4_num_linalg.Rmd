---
title: 'Homework: Numerical linear algebra'
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
    highlight: textmate
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      bA: "{\\boldsymbol{A}}",
      bx: "{\\boldsymbol{x}}",
      bb: "{\\boldsymbol{b}}"
    }
  }
});
</script>


```{r setup, include=FALSE}
source(file.path("..", "R", "util.R"))

required_packages <- c("microbenchmark")
install_and_load_packages(required_packages)
```


# Exercise 1: Comparing different numerical linear algebra algorithms for solving linear systems

In this exercise, we consider the problem of solving a linear system $\bA \bx = \bb$ for $\bx$.
We compare the three methods we learned in the class: LU, Cholesky, and QR decompositions.
(Of course, LU applies to more general systems and QR can be used to solve least squares, but here we focus on positive definite systems.)

## Part A: Racing for solution &mdash; speed comparison 

We first compare their computational speed. 
Fill in the code below and `bench::mark()` the three algorithms.

**Questions:**
What are relative speeds among the algorithms?
Do relative speeds agree with what you expect from the complexity analyses?
If not (quite), why might that be?

**Note:**
I misspoke about the R's built-in `chol()` function during the lecture:
when applied to a positive-definite `A`, the function actually returns an _upper-triangular_ matrix `R` such that `t(R) %*% R == A`.

```{r}
# Import the `rand_positive_def_matrix` function
source(file.path("R", "random_matrix.R"))

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
b <- rnorm(mat_size)
```

```{r}
solve_via_cholesky <- function(A, b) {
  R <- t(chol(A))
  backsolve(t(R), forwardsolve(R, b))
}

solve_via_qr <- function(A, b) {
  # QR <- qr(A)
  # backsolve(qr.R(QR), qr.qty(QR, b))
  solve.qr(qr(A), b, tol = .Machine$double.eps)
}
```

```{r}
bench::mark(
  solve(A, b)
)
```

```{r}
bench::mark(
  solve_via_cholesky(A, b)
)
```

```{r}
bench::mark(
  solve_via_qr(A, b)
)
```
The similarity between runtimes of LU decomposition and cholesky is expected, for LU is dominated by the elimination process which is $\approx \sum_{p'=1}^p  2p'(n-1) \approx O(2/3 p^3)$ here we have $n = p$, which makes it similar to the cholesky decomposition which is $O(1/3 p^3)$ 

The result is not as expected for QR. We expect the complexity of solving linear system Ax = b to be similar for cholesky with $O(1/3p^3)$ and QR decomp with $O(2np^2)$, as here we have matrix dimensions n = p = 1024. However, it seems like QR decomposition is somehow slower than Cholesky and LU here by a factor of roughly 2 - 3 (which remains roughly consistent even when adjusting the dimension of A up or down). Perhaps that factor came from a mix of having to allocate more memory prior to the actual computation and waiting for garbage collection to happen. 

## Part B: Competition in terms of numerical accuracy/stability

We now compare the three methods in terms of numerical accuracy/stability.
To this end, we set up the following simulation study. 
We first generate a "ground truth" solution vector $\bx_0$.
We then compute an "numerical-error-free" $\bb = \bA \bx_0$ by carrying out the matrix-vector multiplication using the `long double` type, which (on most hardware and compilers) provides [additional 12 bits of precision](https://en.wikipedia.org/wiki/Extended_precision#x86_extended_precision_format).
Of course, the vector $\bb$ computed as such still suffers from numerical errors, but the idea is that the numerical errors from this high-precision matrix-vector multiplication is much smaller than the errors caused by numerically solving $\bA \bx = \bb$ for $\bx$.
We can thus assess the accuracy of the three solvers by comparing the numerically computed $\bx$ to the ground truth $\bx_0$.

### Task &#x2F00;

First compare the outputs of matrix-vector multiplication $\bx \to \bA \bx$ using `double` and `long double` using the provided Rcpp functions.

**Questions:**
What is the relative difference in $\ell^2$-norm? 
How about the coordinate-wise relative differences?
Are the observed magnitudes of the differences what you'd expect?

```{r}
Rcpp::sourceCpp(file.path("src", "matvec_double.cpp"))
Rcpp::sourceCpp(file.path("src", "matvec_ldouble.cpp"))

set.seed(1918)

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
x <- rnorm(mat_size)

# Fill in

```


```{r}
pld <- matvec_ldouble(A, x)
pd <- matvec_double(A, x)

(norm(pd - pld, type="2"))/norm(pld, type="2")
```
```{r}
summary(abs((pd - pld)/pld))
```
The relative difference in l2 norm is 5.14 e-16. And the relative coordinate wise difference range from 0 to 9.67 e-14 and 50% of the relative errors fall between 1.78 e-16 to 7.67 e-16.

With 12 extra bits of precision, we expect an additional ability to represent another 4 decimal digits on top of the original 16 (25% more digits represented). And hence what we get from matvec_ldouble should give us the extra precision in at least 3 extra digits. And then when we compare the output of matvec_ldouble and matvec_double we expect to get differences proprotional to the roundoff error around the 16th decimal digit. We are seeing errors on the order of e-14 probably because most output entries from the matvec calculation have 2-3 digits before the decimal point which further eats away precision. 
### Task &#x2F06;

Now randomly generate $\bA$ so that its condition number is $10^6$.
Then solve a positive-definite system $\bA \bx = \bb$ for $\bx$ using the three algorithms and compare their outputs to the ground truth $\bx_0$.

**Questions:**
Which algorithm appears to be more accurate than the others? 
Visually demonstrate your answer.

```{r}
set.seed(1918)
cond_num <- 1e6

# Larger matrices could incur substantial computational time under base R BLAS
mat_size <- 1024L 

A <- rand_positive_def_matrix(mat_size, cond_num)
x <- rnorm(mat_size)


# Fill in

b <- matvec_ldouble(A, x)
```

```{r}
lu_x <- solve(A, b)
chol_x <- solve_via_cholesky(A, b)
qr_x <- solve_via_qr(A, b)
```

```{r}
norm(lu_x-x, type="2")/norm(x, type="2")
norm(chol_x-x, type="2")/norm(x, type="2")
norm(qr_x-x, type="2")/norm(x, type="2")
```

```{r}
library(ggplot2)
library(tidyr)
df <- data.frame(error = c(abs(lu_x - x), abs(chol_x-x), abs(qr_x-x)), 
                  x = c(abs(x), abs(x), abs(x)), 
                  method = c(rep("lu", mat_size), rep("chol", mat_size), 
                             rep("qr", mat_size)) )

df %>% ggplot(aes(x=x, y=error, group=method, color=method)) + geom_point(alpha=0.3) + labs(x="abs(true x)", y="abs(error)")
```

```{r}
df %>% ggplot(aes(x=method, y=error, color=as.factor(method))) + geom_violin() + labs(y = "Absolute coordinate-wise error")
```
Looks like QR is the most accurate, followed by cholesky and then lu comes last place. 

### Task &#x4E09;

In Task &#x2F06;, we compared the three algorithms in one randomly generated example.
Now we consider a more systematic (though hardly comprehensive) comparison via repeated simulations.
We also vary the condition number of $\bA$ and assess whether the results would hold across varying degrees of ill-conditioning.

**Questions/To-do's:**

* Using the starter code provided, calculate various summary measures of the numerical errors.
* Integrate into the provided code one another (or more, if you like) meaningful metric(s) of your choice to summarize the numerical error.
* Visually explore how the three algorithms compare with each other in their accuracy. See if different error metrics tell different stories; they might or might not.
* Vary the condition number in the range $10^6 \sim 10^{12}$, e.g. by trying $10^6$, $10^9$, and $10^{12}$.
* Do you see any patterns in the numerical errors across the three algorithms, metrics, and/or condition numbers? Show some plots to support your conclusion.

**Note:** 
The QR solver will throw an error when the system is ill-condition enough that the numerical solution might not be very accurate. 
To force it to return the solution in any case, set `tol = .Machine$double.eps`.

```{r, cache=TRUE}
# Utility functions for bookkeeping simulation results.
source(file.path("R", "num_linalg_sim_study_helper.R"))

n_sim <- 32L
mat_size <- 512L
set.seed(1918)
cond_num <- 1e6
metrics <- c("norm", "median", "five_percentile", "ninety_five_percentile", "IQR")
  # TODO: add another metric and modify the helper script accordingly

rel_error_list <- lapply(
  c("lu", "chol", "qr"), 
  function(method) pre_allocate_error_list(n_sim, metrics)
)

for (sim_index in 1:n_sim) {
  A <- rand_positive_def_matrix(mat_size, cond_num)
  x <- rnorm(mat_size) 
  b <- matvec_ldouble(A, x)
  x_approx <- list( 
    "lu" = solve(A, b),
    "chol" = solve_via_cholesky(A, b),
    "qr" = solve_via_qr(A, b)
  )
  for (method in c("lu", "chol", "qr")) {
    rel_error <- lapply(
      metrics, 
      function (metric) calc_rel_error(x, x_approx[[method]], metric)
    )
    names(rel_error) <- metrics
    for (metric in names(rel_error)) {
      rel_error_list[[method]][[metric]][sim_index] <- rel_error[[metric]]
    }
  }
}

# TODO: visually compare errors

```

```{r}
library(dplyr)
rel_error_df <- 
  rbind(as.data.frame(rel_error_list[["lu"]]) %>% mutate(method="LU"),
        as.data.frame(rel_error_list[["chol"]]) %>% mutate(method="Chol"),
        as.data.frame(rel_error_list[["qr"]]) %>% mutate(method="QR"))
```

```{r, fig.width=5, fig.height=4}
library(gridExtra)
grid.arrange(
  rel_error_df %>% ggplot(aes(x=method, y=norm)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "l2 norm of relative coordinate-wise Error"), 
  rel_error_df %>% ggplot(aes(x=method, y=median)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "Median of relative coordinate-wise Error"), 
  rel_error_df %>% ggplot(aes(x=method, y=median)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "Median of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=five_percentile)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "5th percentile of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=ninety_five_percentile)) + 
    geom_boxplot() + geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "95th percentile of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=IQR)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "IQR of relative coordinate-wise Error"),ncol=3, nrow=2)
```

```{r, cache=TRUE}
cond_num <- 1e8
set.seed(12123)
rel_error_list <- lapply(
  c("lu", "chol", "qr"), 
  function(method) pre_allocate_error_list(n_sim, metrics)
)

for (sim_index in 1:n_sim) {
  A <- rand_positive_def_matrix(mat_size, cond_num)
  x <- rnorm(mat_size) 
  b <- matvec_ldouble(A, x)
  x_approx <- list( 
    "lu" = solve(A, b),
    "chol" = solve_via_cholesky(A, b),
    "qr" = solve_via_qr(A, b)
  )
  for (method in c("lu", "chol", "qr")) {
    rel_error <- lapply(
      metrics, 
      function (metric) calc_rel_error(x, x_approx[[method]], metric)
    )
    names(rel_error) <- metrics
    for (metric in names(rel_error)) {
      rel_error_list[[method]][[metric]][sim_index] <- rel_error[[metric]]
    }
  }
}
```

```{r}
rel_error_df <- 
  rbind(as.data.frame(rel_error_list[["lu"]]) %>% mutate(method="LU"),
        as.data.frame(rel_error_list[["chol"]]) %>% mutate(method="Chol"),
        as.data.frame(rel_error_list[["qr"]]) %>% mutate(method="QR"))
```

```{r, fig.width=5, fig.height=4}
grid.arrange(
  rel_error_df %>% ggplot(aes(x=method, y=norm)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "l2 norm of relative coordinate-wise Error"), 
  rel_error_df %>% ggplot(aes(x=method, y=median)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "Median of relative coordinate-wise Error"), 
  rel_error_df %>% ggplot(aes(x=method, y=median)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "Median of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=five_percentile)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "5th percentile of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=ninety_five_percentile)) + 
    geom_boxplot() + geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "95th percentile of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=IQR)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "IQR of relative coordinate-wise Error"),ncol=3, nrow=2)
```

```{r, cache=TRUE}
cond_num <- 1e3
set.seed(12123)
rel_error_list <- lapply(
  c("lu", "chol", "qr"), 
  function(method) pre_allocate_error_list(n_sim, metrics)
)

for (sim_index in 1:n_sim) {
  A <- rand_positive_def_matrix(mat_size, cond_num)
  x <- rnorm(mat_size) 
  b <- matvec_ldouble(A, x)
  x_approx <- list( 
    "lu" = solve(A, b),
    "chol" = solve_via_cholesky(A, b),
    "qr" = solve_via_qr(A, b)
  )
  for (method in c("lu", "chol", "qr")) {
    rel_error <- lapply(
      metrics, 
      function (metric) calc_rel_error(x, x_approx[[method]], metric)
    )
    names(rel_error) <- metrics
    for (metric in names(rel_error)) {
      rel_error_list[[method]][[metric]][sim_index] <- rel_error[[metric]]
    }
  }
}
```

```{r}
rel_error_df <- 
  rbind(as.data.frame(rel_error_list[["lu"]]) %>% mutate(method="LU"),
        as.data.frame(rel_error_list[["chol"]]) %>% mutate(method="Chol"),
        as.data.frame(rel_error_list[["qr"]]) %>% mutate(method="QR"))
```

```{r, fig.width=5, fig.height=4}
grid.arrange(
  rel_error_df %>% ggplot(aes(x=method, y=norm)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "l2 norm of relative coordinate-wise Error"), 
  rel_error_df %>% ggplot(aes(x=method, y=median)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "Median of relative coordinate-wise Error"), 
  rel_error_df %>% ggplot(aes(x=method, y=median)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "Median of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=five_percentile)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "5th percentile of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=ninety_five_percentile)) + 
    geom_boxplot() + geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "95th percentile of relative coordinate-wise Error"),
  rel_error_df %>% ggplot(aes(x=method, y=IQR)) + geom_boxplot() + 
    geom_jitter(width = 0.2, alpha=0.3) +
  labs(x = "Method-linear system solve", 
       y = "IQR of relative coordinate-wise Error"),ncol=3, nrow=2)
```
It seems like for large and small condition numbers QR gives the most error on average. Occasionally it achives accuracy comparable to the best results from cholesky and lu but most times it does worse. 
